{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 08:54:40.692653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Coding\")\n",
    "import numpy as np\n",
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        y_pred = self.softmax(z2)\n",
    "        return y_pred\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.concatenate([self.W1.ravel(), self.b1.ravel(), self.W2.ravel(), self.b2.ravel()])\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        sizes = [self.W1.size, self.b1.size, self.W2.size, self.b2.size]\n",
    "        self.W1 = weights[:sizes[0]].reshape(self.W1.shape)\n",
    "        self.b1 = weights[sizes[0]:sizes[0]+sizes[1]].reshape(self.b1.shape)\n",
    "        self.W2 = weights[sizes[0]+sizes[1]:sizes[0]+sizes[1]+sizes[2]].reshape(self.W2.shape)\n",
    "        self.b2 = weights[-sizes[3]:].reshape(self.b2.shape)\n",
    "        \n",
    "        \n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.sum(y_true * np.log(y_pred))\n",
    "    return loss / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and data\n",
    "\n",
    "X = x_train.reshape(-1, 784)\n",
    "Y = np.eye(10)[y_train]  # one-hot encoding, e.g. label 4 -> [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]\n",
    "\n",
    "\n",
    "mlp = MLP(input_size=784, hidden_size=16, output_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta (12730,) weight magnitude:  0.008048773592840132\n",
      "Iteration: 1/100, mean accuracy: 0.1305, mean loss: 0.2395050756748789 , gradient magnitude 0.2795705629181174, gradient mean -3.1312425846834915e-05\n",
      "Iteration: 2/100, mean accuracy: 0.1409, mean loss: 0.23519497253207083 , gradient magnitude 0.2592709063556789, gradient mean 1.7426001046042663e-05\n",
      "Iteration: 3/100, mean accuracy: 0.1463, mean loss: 0.23283764341287097 , gradient magnitude 0.31332735273692586, gradient mean 2.0504755285732186e-05\n",
      "Iteration: 4/100, mean accuracy: 0.1563, mean loss: 0.2306572989573046 , gradient magnitude 0.24633108931448938, gradient mean -2.2089311668897614e-05\n",
      "Iteration: 5/100, mean accuracy: 0.1752, mean loss: 0.22679559240611816 , gradient magnitude 0.25532838377707295, gradient mean 3.836595452630927e-05\n",
      "Iteration: 6/100, mean accuracy: 0.1803, mean loss: 0.22310493152660785 , gradient magnitude 0.26960190379325916, gradient mean -1.1896332252146087e-05\n",
      "Iteration: 7/100, mean accuracy: 0.2046, mean loss: 0.21770529739019404 , gradient magnitude 0.24789385459597213, gradient mean 2.9287737503589028e-05\n",
      "Iteration: 8/100, mean accuracy: 0.2197, mean loss: 0.21415635173146622 , gradient magnitude 0.2820389881606674, gradient mean 2.6731188103383258e-05\n",
      "Iteration: 9/100, mean accuracy: 0.2594, mean loss: 0.20765704943561922 , gradient magnitude 0.2595753025849178, gradient mean 3.947928788450744e-05\n",
      "Iteration: 10/100, mean accuracy: 0.2877, mean loss: 0.20462091002140997 , gradient magnitude 0.24708508779405575, gradient mean 1.1477645798920685e-05\n",
      "Iteration: 11/100, mean accuracy: 0.3093, mean loss: 0.20088793597143384 , gradient magnitude 0.29000510172450794, gradient mean -3.748433978459217e-05\n",
      "Iteration: 12/100, mean accuracy: 0.3231, mean loss: 0.19775397092079916 , gradient magnitude 0.25761197646958134, gradient mean -6.537579833774701e-06\n",
      "Iteration: 13/100, mean accuracy: 0.3400, mean loss: 0.1937107101698929 , gradient magnitude 0.26583901122051645, gradient mean 1.826640974811041e-06\n",
      "Iteration: 14/100, mean accuracy: 0.3533, mean loss: 0.19126287724891766 , gradient magnitude 0.2516907964528572, gradient mean 2.1763074775812015e-05\n",
      "Iteration: 15/100, mean accuracy: 0.3584, mean loss: 0.187412695618212 , gradient magnitude 0.2652005092044737, gradient mean -3.396172435255063e-05\n",
      "Iteration: 16/100, mean accuracy: 0.3810, mean loss: 0.18193081486647208 , gradient magnitude 0.2564489338299854, gradient mean -3.664233253108161e-05\n",
      "Iteration: 17/100, mean accuracy: 0.3851, mean loss: 0.18006620258267375 , gradient magnitude 0.261452552015122, gradient mean 5.029548884162612e-05\n",
      "Iteration: 18/100, mean accuracy: 0.3991, mean loss: 0.17845269511072817 , gradient magnitude 0.2616556233530291, gradient mean -2.5330082414614107e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# set weights an compute forwardpass + loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m mlp\u001b[38;5;241m.\u001b[39mset_weights(theta \u001b[38;5;241m+\u001b[39m epsilon)\n\u001b[0;32m---> 27\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m cross_entropy \u001b[38;5;241m=\u001b[39m cross_entropy_loss(y_pred, Y[idx])\n\u001b[1;32m     30\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(cross_entropy)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(z1)\n\u001b[1;32m     18\u001b[0m z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2\n\u001b[0;32m---> 19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mMLP.softmax\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     exp_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exp_x \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# these parameters are not really tuned, feel free to improve!\n",
    "\n",
    "iterations = 100\n",
    "population_size = 2500\n",
    "sigma = 1e-3    # we want the purtubation strength in the same ballpark as the weights\n",
    "learning_rate = 0.5\n",
    "batch_size = 16\n",
    "\n",
    "theta = mlp.get_weights()\n",
    "\n",
    "print(\"theta\", theta.shape, 'weight magnitude: ', np.mean(np.abs(theta)))\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    directions = []\n",
    "    loss = []\n",
    "    accuracy = []\n",
    "    for individual in range(population_size):\n",
    "        # generate random pertubation\n",
    "        epsilon =  np.random.normal(0, sigma, size=theta.shape)\n",
    "        losses = []\n",
    "        for b in range(batch_size):\n",
    "            # evaluate on a batch of random images\n",
    "            idx = np.random.randint(0, len(X) - 1)\n",
    "            sample = X[idx]\n",
    "            # set weights an compute forwardpass + loss\n",
    "            mlp.set_weights(theta + epsilon)\n",
    "            y_pred = mlp.forward(sample)\n",
    "            cross_entropy = cross_entropy_loss(y_pred, Y[idx])\n",
    "            \n",
    "            losses.append(cross_entropy)\n",
    "            accuracy.append(np.argmax(y_pred) == y_train[idx])\n",
    "    \n",
    "        directions.append(epsilon)\n",
    "        loss.append(np.mean(losses))\n",
    "\n",
    "    directions = np.array(directions)\n",
    "    loss = np.array(loss)\n",
    "    \n",
    "    # normalize to -0.5 to 0.5\n",
    "    fitness = ((loss - loss.min()) / (loss.max() - loss.min())) - 0.5\n",
    "\n",
    "    # estimate gradient by multiplying the random directions with the found loss decrease\n",
    "    #  note that this includes the directions that turned out to increase the loss, but\n",
    "    #  since the values are normalized between -0.5 and +0.5, they will end up with a \n",
    "    #  negative sign so everything works out\n",
    "    gradient = np.mean(np.multiply(directions.T, fitness), axis=1) / sigma\n",
    "    # bonus question: why devide by sigma? \n",
    "\n",
    "    # use the gradient for gradient descent\n",
    "    theta = theta - learning_rate * gradient\n",
    "\n",
    "    # print(len(theta))\n",
    "\n",
    "    print(f\"Iteration: {iteration+1}/{iterations}, mean accuracy: {np.mean(accuracy):.4f}, mean loss: {np.mean(loss)}\", end='')\n",
    "    print(f\" , gradient magnitude {np.mean(np.abs(gradient))/np.mean(np.abs(theta))}, gradient mean {np.mean(gradient)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
