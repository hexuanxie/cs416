{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 16:20:11.414625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import random\n",
    "\n",
    "import keras\n",
    "(train_inputs, train_labels), (test_inputs, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "train_inputs = train_inputs.astype(\"float32\") / 255.0\n",
    "test_inputs = test_inputs.astype(\"float32\") / 255.0\n",
    "\n",
    "test_inputs = test_inputs.astype(\"int32\")\n",
    "test_inputs = test_inputs.astype(\"int32\")\n",
    "\n",
    "training_data = [np.array(x.flatten()) for x in train_inputs]\n",
    "test_data = [np.array(x.flatten()) for x in test_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.2        0.62352943 0.99215686 0.62352943 0.19607843\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1882353  0.93333334\n",
      " 0.9882353  0.9882353  0.9882353  0.92941177 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.21176471 0.8901961  0.99215686 0.9882353  0.9372549\n",
      " 0.9137255  0.9882353  0.22352941 0.02352941 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03921569 0.23529412 0.8784314\n",
      " 0.9882353  0.99215686 0.9882353  0.7921569  0.32941177 0.9882353\n",
      " 0.99215686 0.47843137 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6392157  0.9882353  0.9882353  0.9882353  0.99215686\n",
      " 0.9882353  0.9882353  0.3764706  0.7411765  0.99215686 0.654902\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.2        0.93333334\n",
      " 0.99215686 0.99215686 0.74509805 0.44705883 0.99215686 0.89411765\n",
      " 0.18431373 0.30980393 1.         0.65882355 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.1882353  0.93333334 0.9882353  0.9882353  0.7019608\n",
      " 0.04705882 0.29411766 0.4745098  0.08235294 0.         0.\n",
      " 0.99215686 0.9529412  0.19607843 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.64705884\n",
      " 0.99215686 0.9137255  0.8156863  0.32941177 0.         0.\n",
      " 0.         0.         0.         0.         0.99215686 0.9882353\n",
      " 0.64705884 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02745098 0.69803923 0.9882353  0.9411765  0.2784314\n",
      " 0.07450981 0.10980392 0.         0.         0.         0.\n",
      " 0.         0.         0.99215686 0.9882353  0.7647059  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.22352941\n",
      " 0.9882353  0.9882353  0.24705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.99215686 0.9882353  0.7647059  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.7764706  0.99215686 0.74509805\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.99215686\n",
      " 0.76862746 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.29803923 0.9647059  0.9882353  0.4392157  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99215686 0.9882353  0.5803922  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.33333334 0.9882353\n",
      " 0.9019608  0.09803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02745098 0.5294118\n",
      " 0.99215686 0.7294118  0.04705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333334 0.9882353  0.8745098  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02745098 0.5137255  0.9882353  0.88235295 0.2784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333334 0.9882353  0.5686275  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1882353  0.64705884\n",
      " 0.9882353  0.6784314  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3372549  0.99215686\n",
      " 0.88235295 0.         0.         0.         0.         0.\n",
      " 0.         0.44705883 0.93333334 0.99215686 0.63529414 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333334 0.9882353  0.9764706  0.57254905\n",
      " 0.1882353  0.11372549 0.33333334 0.69803923 0.88235295 0.99215686\n",
      " 0.8745098  0.654902   0.21960784 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333334 0.9882353  0.9882353  0.9882353  0.8980392  0.84313726\n",
      " 0.9882353  0.9882353  0.9882353  0.76862746 0.50980395 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.10980392 0.78039217\n",
      " 0.9882353  0.9882353  0.99215686 0.9882353  0.9882353  0.9137255\n",
      " 0.5686275  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09803922 0.5019608  0.9882353\n",
      " 0.99215686 0.9882353  0.5529412  0.14509805 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers):\n",
    "        # layers: list with how many neurons in each layer\n",
    "        self.layers = layers\n",
    "        self.weights = np.random.rand(10, 784) * 0.001\n",
    "        # assuming a bias of 0\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # x: a 1d np.array\n",
    "        # returns the softmax value\n",
    "        x = x - np.max(x)\n",
    "        exp_x = np.exp(x)\n",
    "        softmax_x = exp_x / np.sum(exp_x)\n",
    "        return softmax_x\n",
    "\n",
    "    def randomize_array(self, arr):\n",
    "        # returns a slightly randomized, noisy np.array of the input np.array\n",
    "        noise = np.random.normal(0, 0.5, arr.shape)\n",
    "        randomized_arr = arr + noise\n",
    "        return randomized_arr\n",
    "\n",
    "    def vectorized_result(self, x):\n",
    "        # returns a np.array with 0, except for a 1 in the x index\n",
    "        return_vec = np.zeros(10)\n",
    "        return_vec[int(x)] = 1\n",
    "        return return_vec\n",
    "\n",
    "    def forward_pass(self, input, weights):\n",
    "        return self.softmax(np.dot(weights, input))\n",
    "\n",
    "    def identify(self, input):\n",
    "        return np.argmax(input)\n",
    "\n",
    "    def cross_entropy_loss(self, actual, prediction):\n",
    "        epsilon = 1e-15  # to prevent log(0)\n",
    "        y_pred = np.clip(prediction, epsilon, 1 - epsilon)\n",
    "        return -np.mean(actual * np.log(y_pred) + (1 - actual) * np.log(1 - y_pred))\n",
    "\n",
    "    def train(self, training_data, labels):\n",
    "        pairs = list(zip(training_data, labels))\n",
    "        for i in range(100): # set number of epochs to 100\n",
    "            random.shuffle(pairs)\n",
    "            n = len(pairs)\n",
    "            mini_batches = [pairs[k:k+2500] for k in range(0, n, 2500)] # set batch size to 100 to train\n",
    "            weights_list = []\n",
    "            loss_list = []\n",
    "            for data, label in mini_batches[0]:\n",
    "                actual_vector = self.vectorized_result(label)\n",
    "                # bias the weight matrices 3 times and multiply by error and find the mean:\n",
    "                new_weight_list = []\n",
    "                new_loss_list = []\n",
    "\n",
    "                # add the current weights\n",
    "                pred = self.softmax(self.forward_pass(data, self.weights))\n",
    "                this_loss = self.cross_entropy_loss(pred, actual_vector)\n",
    "                new_loss_list.append(this_loss)\n",
    "                new_weight_list.append(self.weights)\n",
    "                # create slight variations and evolutions\n",
    "                for j in range(5):\n",
    "                    curr_weight = self.randomize_array(self.weights)\n",
    "                    new_weight_list.append(curr_weight)\n",
    "                    prediction = self.softmax(self.forward_pass(data, curr_weight))\n",
    "                    loss = self.cross_entropy_loss(prediction, actual_vector)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                # pick best evolution or curent weight, that is the one with the least amount of loss\n",
    "                new_weight_array = np.array(new_weight_list)\n",
    "                new_loss_array = np.array(new_loss_list)\n",
    "                index = np.argmin(new_loss_array)\n",
    "                weights_list.append(new_weight_array[index] - self.weights) # updated to only add the noise vector\n",
    "                loss_list.append(new_loss_array[index])\n",
    "\n",
    "            weight_array = np.array(weights_list)\n",
    "            loss_array = np.array(loss_list)\n",
    "            normalized = ((loss_array - loss_array.min()) / (loss_array.max() - loss_array.min())) - 0.5\n",
    "            gradient = np.zeros(weight_array[0].shape)\n",
    "            for j in range(len(weights_list)):\n",
    "                gradient += weight_array[j] * normalized[j]\n",
    "            gradient /= len(weights_list)\n",
    "            self.weights -= 0.01 * gradient # learning rate = 0.01\n",
    "\n",
    "    def test_accuracy(self):\n",
    "        total_num_trials = 0\n",
    "        correct = 0\n",
    "        pairs = zip(test_data, test_labels)\n",
    "        for input, label in pairs:\n",
    "            x = self.forward_pass(input, self.weights)\n",
    "            id = self.identify(x)\n",
    "            if (id == label):\n",
    "                correct += 1\n",
    "            total_num_trials += 1\n",
    "        return (correct / total_num_trials)\n",
    "        # print(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Accuracy: 9.66%\n",
      "After Training Accuracy: 13.309999999999999%\n",
      "\n",
      "\n",
      "Original Accuracy: 11.31%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet\u001b[38;5;241m.\u001b[39mtest_accuracy()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet\u001b[38;5;241m.\u001b[39mtest_accuracy()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[0;34m(self, training_data, labels)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# create slight variations and evolutions\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     curr_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     new_weight_list\u001b[38;5;241m.\u001b[39mappend(curr_weight)\n\u001b[1;32m     68\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pass(data, curr_weight))\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mNeuralNet.randomize_array\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandomize_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, arr):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# returns a slightly randomized, noisy np.array of the input np.array\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     noise \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     randomized_arr \u001b[38;5;241m=\u001b[39m arr \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m randomized_arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    net = NeuralNet([784, 10])\n",
    "    print()\n",
    "    print(f\"Original Accuracy: {net.test_accuracy() * 100}%\")\n",
    "    net.train(training_data, train_labels)\n",
    "    print(f\"After Training Accuracy: {net.test_accuracy() * 100}%\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
